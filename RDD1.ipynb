{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.streaming import StreamingContext\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "from kafka import KafkaProducer\n",
    "import logging\n",
    "from json import dumps, loads\n",
    "import csv\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "from kafka import KafkaConsumer\n",
    "from kafka import KafkaProducer\n",
    "from kafka import TopicPartition\n",
    "import json\n",
    "from data import get_registered_user\n",
    "import time\n",
    "from pyspark.sql.functions import from_json, col, to_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder\\\n",
    "        .appName(\"DataframeABC\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identifed as 1.0.0\n",
      "INFO:kafka.conn:Set configuration api_version=(1, 0, 0) to skip auto check_version requests on startup\n",
      "INFO:kafka.producer.kafka:Closing the Kafka producer with 0 secs timeout.\n",
      "INFO:kafka.producer.kafka:Proceeding to force close the producer since pending requests could not be completed within timeout 0.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=ML-RefVm-639429.o4kzperxrtjere1xbec2ne5kif.jx.internal.cloudapp.net:9092 <connected> [IPv4 ('10.20.0.43', 9092)]>: Closing connection. \n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=ML-RefVm-639429.o4kzperxrtjere1xbec2ne5kif.jx.internal.cloudapp.net:9092 <connecting> [IPv4 ('10.20.0.43', 9092)]>: connecting to ML-RefVm-639429.o4kzperxrtjere1xbec2ne5kif.jx.internal.cloudapp.net:9092 [('10.20.0.43', 9092) IPv4]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=ML-RefVm-639429.o4kzperxrtjere1xbec2ne5kif.jx.internal.cloudapp.net:9092 <connecting> [IPv4 ('10.20.0.43', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataFA = pd.read_csv ('KCLT.csv')\n",
    "dataFB = pd.read_csv ('KCQT.csv')\n",
    "dataFC = pd.read_csv ('KHOU.csv')\n",
    "dataFD = pd.read_csv ('KIND.csv')\n",
    "dataFE = pd.read_csv ('KJAX.csv')\n",
    "dataFF = pd.read_csv ('KMDW.csv')\n",
    "dataFG = pd.read_csv ('KNYC.csv')\n",
    "dataFH = pd.read_csv ('KPHL.csv')\n",
    "dataFI = pd.read_csv ('KPHX.csv')\n",
    "dataFJ = pd.read_csv ('KSEA.csv')\n",
    "\n",
    "dataFA2 = dataFA.to_json()\n",
    "dataFB2 = dataFB.to_json()\n",
    "dataFC2 = dataFC.to_json()\n",
    "dataFD2 = dataFD.to_json()\n",
    "dataFE2 = dataFE.to_json()\n",
    "dataFF2 = dataFF.to_json()\n",
    "dataFG2 = dataFG.to_json()\n",
    "dataFH2 = dataFH.to_json()\n",
    "dataFI2 = dataFI.to_json()\n",
    "dataFJ2 = dataFJ.to_json()\n",
    "#initialize producer\n",
    "producer = KafkaProducer(bootstrap_servers=['localhost:9092'], \n",
    "value_serializer=lambda v:dumps(v).encode('utf-8'))\n",
    "#key_serializer=lambda v:dumps(v).encode('utf-8')\n",
    "sleep_time=2\n",
    "# producer.send('TestTopic4', key='A', value=dataFA2)\n",
    "# producer.send('TestTopic4', key='B',value=dataFA2)\n",
    "# producer.send('TestTopic4', key='C',value=dataFA2)\n",
    "# producer.send('TestTopic4', key='D',value=dataFA2)\n",
    "# producer.send('TestTopic4', key='E',value=dataFA2)\n",
    "# producer.send('TestTopic4', key='F',value=dataFA2)\n",
    "# producer.send('TestTopic4', key='G',value=dataFA2)\n",
    "# producer.send('TestTopic4', key='H',value=dataFA2)\n",
    "# producer.send('TestTopic4', key='I',value=dataFA2)\n",
    "producer.send('TestTopic4',value=dataFA2)\n",
    "producer.send('TestTopic5',value=dataFB2)\n",
    "producer.send('TestTopic6',value=dataFC2)\n",
    "producer.send('TestTopic7',value=dataFD2)\n",
    "producer.send('TestTopic8',value=dataFE2)\n",
    "producer.send('TestTopic9',value=dataFF2)\n",
    "producer.send('TestTopic10',value=dataFG2)\n",
    "producer.send('TestTopic11',value=dataFH2)\n",
    "producer.send('TestTopic12',value=dataFI2)\n",
    "producer.send('TestTopic13',value=dataFJ2)\n",
    "time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]\n",
      "INFO:kafka.conn:Probing node bootstrap-0 broker version\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.\n",
      "INFO:kafka.conn:Broker version identifed as 1.0.0\n",
      "INFO:kafka.conn:Set configuration api_version=(1, 0, 0) to skip auto check_version requests on startup\n",
      "INFO:kafka.consumer.subscription_state:Updating subscribed topics to: ('TestTopic4',)\n",
      "INFO:kafka.coordinator:Stopping heartbeat thread\n"
     ]
    }
   ],
   "source": [
    "group_id=\"vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\"\n",
    "if __name__ == \"__main__\":\n",
    "    consumer = KafkaConsumer('TestTopic4',\n",
    "    bootstrap_servers = \"localhost:9092\",\n",
    "    value_deserializer = lambda v: json.loads(v.decode('utf-8')),\n",
    "    #key_serializer=lambda v:dumps(v).encode('utf-8')),\n",
    "    group_id=group_id,\n",
    "    auto_commit_interval_ms=1000,\n",
    "    auto_offset_reset='latest',\n",
    "    consumer_timeout_ms=1000\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.consumer.subscription_state:subscription unchanged by change_subscription(['TestTopic4'])\n",
      "INFO:kafka.cluster:Group coordinator for vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 is BrokerMetadata(nodeId='coordinator-0', host='ML-RefVm-639429.o4kzperxrtjere1xbec2ne5kif.jx.internal.cloudapp.net', port=9092, rack=None)\n",
      "INFO:kafka.coordinator:Discovered coordinator coordinator-0 for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Starting new heartbeat thread\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.conn:<BrokerConnection node_id=coordinator-0 host=ML-RefVm-639429.o4kzperxrtjere1xbec2ne5kif.jx.internal.cloudapp.net:9092 <connecting> [IPv4 ('10.20.0.43', 9092)]>: connecting to ML-RefVm-639429.o4kzperxrtjere1xbec2ne5kif.jx.internal.cloudapp.net:9092 [('10.20.0.43', 9092) IPv4]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=coordinator-0 host=ML-RefVm-639429.o4kzperxrtjere1xbec2ne5kif.jx.internal.cloudapp.net:9092 <connecting> [IPv4 ('10.20.0.43', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. \n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: connecting to localhost:9092 [('::1', 9092, 0, 0) IPv6]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connecting> [IPv6 ('::1', 9092, 0, 0)]>: Connection complete.\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=ML-RefVm-639429.o4kzperxrtjere1xbec2ne5kif.jx.internal.cloudapp.net:9092 <connecting> [IPv4 ('10.20.0.43', 9092)]>: connecting to ML-RefVm-639429.o4kzperxrtjere1xbec2ne5kif.jx.internal.cloudapp.net:9092 [('10.20.0.43', 9092) IPv4]\n",
      "INFO:kafka.conn:<BrokerConnection node_id=0 host=ML-RefVm-639429.o4kzperxrtjere1xbec2ne5kif.jx.internal.cloudapp.net:9092 <connecting> [IPv4 ('10.20.0.43', 9092)]>: Connection complete.\n",
      "INFO:kafka.conn:<BrokerConnection node_id=bootstrap-0 host=localhost:9092 <connected> [IPv6 ('::1', 9092, 0, 0)]>: Closing connection. \n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 136\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic='TestTopic4', partition=0)]\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic='TestTopic4', partition=0)} for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.consumer.subscription_state:Updating subscribed topics to: ['TestTopic5']\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "WARNING:kafka.coordinator.assignors.range:No partition metadata for topic TestTopic5\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 137\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: []\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 138\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic='TestTopic5', partition=0)]\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic='TestTopic5', partition=0)} for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.consumer.subscription_state:Updating subscribed topics to: ['TestTopic6']\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "WARNING:kafka.coordinator.assignors.range:No partition metadata for topic TestTopic6\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 139\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: []\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 140\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic='TestTopic6', partition=0)]\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic='TestTopic6', partition=0)} for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.consumer.subscription_state:Updating subscribed topics to: ['TestTopic7']\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "WARNING:kafka.coordinator.assignors.range:No partition metadata for topic TestTopic7\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 141\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: []\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 142\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic='TestTopic7', partition=0)]\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic='TestTopic7', partition=0)} for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.consumer.subscription_state:Updating subscribed topics to: ['TestTopic8']\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "WARNING:kafka.coordinator.assignors.range:No partition metadata for topic TestTopic8\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 143\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: []\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 144\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic='TestTopic8', partition=0)]\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic='TestTopic8', partition=0)} for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.consumer.subscription_state:Updating subscribed topics to: ['TestTopic9']\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "WARNING:kafka.coordinator.assignors.range:No partition metadata for topic TestTopic9\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 145\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: []\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 146\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic='TestTopic9', partition=0)]\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic='TestTopic9', partition=0)} for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.consumer.subscription_state:Updating subscribed topics to: ['TestTopic10']\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "WARNING:kafka.coordinator.assignors.range:No partition metadata for topic TestTopic10\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 147\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: []\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 148\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic='TestTopic10', partition=0)]\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic='TestTopic10', partition=0)} for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.consumer.subscription_state:Updating subscribed topics to: ['TestTopic11']\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "WARNING:kafka.coordinator.assignors.range:No partition metadata for topic TestTopic11\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 149\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: []\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 150\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic='TestTopic11', partition=0)]\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic='TestTopic11', partition=0)} for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.consumer.subscription_state:Updating subscribed topics to: ['TestTopic12']\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "WARNING:kafka.coordinator.assignors.range:No partition metadata for topic TestTopic12\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 151\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: []\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 152\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic='TestTopic12', partition=0)]\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic='TestTopic12', partition=0)} for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.consumer.subscription_state:Updating subscribed topics to: ['TestTopic13']\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "WARNING:kafka.coordinator.assignors.range:No partition metadata for topic TestTopic13\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 153\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: []\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator.consumer:Revoking previously assigned partitions set() for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:(Re-)joining group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n",
      "INFO:kafka.coordinator:Elected group leader -- performing partition assignments using range\n",
      "INFO:kafka.coordinator:Successfully joined group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4 with generation 154\n",
      "INFO:kafka.consumer.subscription_state:Updated partition assignment: [TopicPartition(topic='TestTopic13', partition=0)]\n",
      "INFO:kafka.coordinator.consumer:Setting newly assigned partitions {TopicPartition(topic='TestTopic13', partition=0)} for group vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4\n"
     ]
    }
   ],
   "source": [
    "# l=[]\n",
    "# for xvar in consumer:\n",
    "#     l.append(dataFA)\n",
    "# df = pd.DataFrame(l)\n",
    "\n",
    "# Xval = spark.readStream \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#     .option(\"subscribe\", \"TestTopic4\") \\\n",
    "#     .option(\"startingOffsets\", \"latest\") \n",
    "\n",
    "consumer.subscribe(topics='TestTopic4')\n",
    "message_listA=[]\n",
    "for message in consumer:\n",
    "    msg_str=message\n",
    "    message_listA.append(msg_str)\n",
    "\n",
    "consumer.subscribe(topics='TestTopic5')\n",
    "message_listB=[]\n",
    "for message in consumer:\n",
    "    msg_str=message\n",
    "    message_listB.append(msg_str)\n",
    "\n",
    "consumer.subscribe(topics='TestTopic6')\n",
    "message_listC=[]\n",
    "for message in consumer:\n",
    "    msg_str=message\n",
    "    message_listC.append(msg_str)\n",
    "\n",
    "consumer.subscribe(topics='TestTopic7')\n",
    "message_listD=[]\n",
    "for message in consumer:\n",
    "    msg_str=message\n",
    "    message_listD.append(msg_str)\n",
    "\n",
    "consumer.subscribe(topics='TestTopic8')\n",
    "message_listE=[]\n",
    "for message in consumer:\n",
    "    msg_str=message\n",
    "    message_listE.append(msg_str)\n",
    "\n",
    "consumer.subscribe(topics='TestTopic9')\n",
    "message_listF=[]\n",
    "for message in consumer:\n",
    "    msg_str=message\n",
    "    message_listF.append(msg_str)\n",
    "\n",
    "consumer.subscribe(topics='TestTopic10')\n",
    "message_listG=[]\n",
    "for message in consumer:\n",
    "    msg_str=message\n",
    "    message_listG.append(msg_str)\n",
    "\n",
    "consumer.subscribe(topics='TestTopic11')\n",
    "message_listH=[]\n",
    "for message in consumer:\n",
    "    msg_str=message\n",
    "    message_listH.append(msg_str)\n",
    "\n",
    "consumer.subscribe(topics='TestTopic12')\n",
    "message_listI=[]\n",
    "for message in consumer:\n",
    "    msg_str=message\n",
    "    message_listI.append(msg_str)\n",
    "\n",
    "consumer.subscribe(topics='TestTopic13')\n",
    "message_listJ=[]\n",
    "for message in consumer:\n",
    "    msg_str=message\n",
    "    message_listJ.append(msg_str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for dataFA2 in consumer:\n",
    "#     json1 = pd.json_normalize(dataFA2)\n",
    "#     dfA = pd.DataFrame(json1)\n",
    "#     dfA = spark.createDataFrame(dfA)\n",
    "#     dfA = dfA.append(json1.value)\n",
    "message_listA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark #this is reading in memory with one master node\n",
    "df = dataFA\n",
    "df2 = dataFA\n",
    "df3 = dataFB\n",
    "df4 = dataFC\n",
    "df5 = dataFD\n",
    "df6 = dataFE\n",
    "df7 = dataFF\n",
    "df8 = dataFG\n",
    "df9 = dataFH\n",
    "df10 = dataFI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark #this is reading in memory with one master node\n",
    "df = dataFA\n",
    "df2 = dataFA\n",
    "df3 = dataFB\n",
    "df4 = dataFC\n",
    "df5 = dataFD\n",
    "df6 = dataFE\n",
    "df7 = dataFF\n",
    "df8 = dataFG\n",
    "df9 = dataFH\n",
    "df10 = dataFI\n",
    "df=spark.createDataFrame(df)\n",
    "df2=spark.createDataFrame(df2)\n",
    "df3=spark.createDataFrame(df3)\n",
    "df4=spark.createDataFrame(df4)\n",
    "df5=spark.createDataFrame(df5)\n",
    "df6=spark.createDataFrame(df6)\n",
    "df7=spark.createDataFrame(df7)\n",
    "df8=spark.createDataFrame(df8)\n",
    "df9=spark.createDataFrame(df9)\n",
    "df10=spark.createDataFrame(df10)\n",
    "# df=spark.read.option(\"header\",True).csv('KCQT.csv')\n",
    "# df2=spark.read.option(\"header\",True).csv('KCQT.csv')\n",
    "# df3=spark.read.option(\"header\",True).csv('KHOU.csv')\n",
    "# df4=spark.read.option(\"header\",True).csv('KIND.csv')\n",
    "# df5=spark.read.option(\"header\",True).csv('KJAX.csv')\n",
    "# df6=spark.read.option(\"header\",True).csv('KMDW.csv')\n",
    "# df7=spark.read.option(\"header\",True).csv('KNYC.csv')\n",
    "# df8=spark.read.option(\"header\",True).csv('KPHL.csv')\n",
    "# df9=spark.read.option(\"header\",True).csv('KPHX.csv')\n",
    "# df10=spark.read.option(\"header\",True).csv('KSEA.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show()\n",
    "#df.orderBy(\"date\")\n",
    "#df.select(F.date_format('date','yyyy-MM-dd').alias('day')).groupby('day').count().show()\n",
    "#from pyspark.sql import functions as F\n",
    "#df.select(F.date_format('date','yyyy-MM-dd').alias('week')).groupby('week').count().show()\n",
    "#df.write.options(header='True').csv(\"/tmp/resources/test1.csv\")\n",
    "#variableA.select(F.date_format('date','yyyy-MM-dd').alias('week')).groupby('week').count().show()\n",
    "#variableA = variableA.withColumn(\"Week\",col(\"actual_min_temp\")*0+1)\n",
    "#variableA = variableA.withColumn(\"Week\",variableA.Week.cast('integer'))\n",
    "#variableA = variableA.select(variableA.columns[:6])\n",
    "#variableA3 = variableA3.withColumn(\"date\",col(\"actual_min_temp\")*0+1)\n",
    "#limits the number of rows to the top\n",
    "#variableA = variableA.withColumn(\"Week\",\n",
    "#    when(variableA.date == '2014-7-1',\"week1\")\n",
    "#    .when(variableA.date == '2014-7-2',\"week1\")\n",
    "#    .when(variableA.date == '2014-7-3',\"week1\")\n",
    "#    .when(variableA.date == '2014-7-4',\"week1\")\n",
    "#    .when(variableA.date == '2014-7-5',\"week1\")\n",
    "#    .when(variableA.date == '2014-7-6',\"week1\")\n",
    "#    .when(variableA.date == '2014-7-7',\"week1\"))\n",
    "#limits the number of rows to the top\n",
    "#df7rows = variableA.limit(7).withColumn('Week', col(\"Week\"))\n",
    "#df7rows = df7rows.withColumn('Week',\n",
    "#    when(df7rows.Week == '1',\"week1\"))\n",
    "#df7rows = df7rows.sort(col(\"date\"))\n",
    "#variableB = variableA.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp').subtract(df7rows.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp'))\n",
    "#variableA = variableA.withColumn(\"Week\", when(variableA.date == \"2014-7-1\",\"week1\").otherwise(variableA.date))\n",
    "#variableB.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F, Window\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import hour, mean, weekofyear\n",
    "\n",
    "def passA(x, dataframeVar):\n",
    "    if x == 1:\n",
    "        dataframeVar = df\n",
    "    if x == 2:\n",
    "        dataframeVar = df2\n",
    "    if x == 3:\n",
    "        dataframeVar = df3\n",
    "    if x == 4:\n",
    "        dataframeVar = df4\n",
    "    if x == 5:\n",
    "        dataframeVar = df5\n",
    "    if x == 6:\n",
    "        dataframeVar = df6\n",
    "    if x == 7:\n",
    "        dataframeVar = df7\n",
    "    if x == 8:\n",
    "        dataframeVar = df8\n",
    "    if x == 9:\n",
    "        dataframeVar = df9\n",
    "    if x == 10:\n",
    "        dataframeVar = df10\n",
    "    return(dataframeVar, x)\n",
    "\n",
    "def func1(dataframeVar):\n",
    "    variableA = dataframeVar.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp')\n",
    "    variableA = variableA.withColumn(\"DayOfWeek\", dayofweek(col(\"date\")))\n",
    "    variableA = (variableA\n",
    "        .groupBy(weekofyear(\"date\"))\n",
    "        .agg(mean('actual_min_temp'),mean('actual_max_temp'),mean('average_min_temp'),mean('average_max_temp'))\n",
    "        .orderBy(weekofyear(\"date\"))\n",
    "    )\n",
    "    #variableA.coalesce(1).write.options(header='True').csv(\"C:/Users/dsvm_user/data/1KCLT.csv\")\n",
    "    return(variableA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/C:/Users/dsvm_user/data/final/data/KCLT_weekly.csv already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-a650d4b6521a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mvariableA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariableA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DayOfWeek\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdayofweek\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mvariableA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvariableA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweekofyear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'actual_min_temp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'actual_max_temp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'average_min_temp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'average_max_temp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweekofyear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"date\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mvariableA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'True'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/dsvm_user/data/final/data/KCLT_weekly.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mvardf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariableA\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dsvm\\tools\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1369\u001b[0m                        \u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcharToEscapeQuoteEscaping\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1370\u001b[0m                        encoding=encoding, emptyValue=emptyValue, lineSep=lineSep)\n\u001b[1;32m-> 1371\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1373\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0morc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitionBy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Miniconda\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\dsvm\\tools\\spark-3.1.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: path file:/C:/Users/dsvm_user/data/final/data/KCLT_weekly.csv already exists."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:kafka.coordinator:Heartbeat poll expired, leaving group\n",
      "INFO:kafka.coordinator:Leaving consumer group (vscode-kafka-Cluser1Temp-bG9jYWxob3N0OjkwOTI-TestTopic4).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataframeVar = df\n",
    "variableA = dataframeVar.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp')\n",
    "variableA = variableA.withColumn(\"DayOfWeek\", dayofweek(col(\"date\")))\n",
    "variableA = (variableA.groupBy(weekofyear(\"date\")).agg(mean('actual_min_temp'),mean('actual_max_temp'),mean('average_min_temp'),mean('average_max_temp')).orderBy(weekofyear(\"date\")))\n",
    "variableA.coalesce(1).write.options(header='True').csv(\"C:/Users/dsvm_user/data/final/data/KCLT_weekly.csv\")\n",
    "vardf = variableA\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "dataframeVar = df2\n",
    "variableA = dataframeVar.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp')\n",
    "variableA = variableA.withColumn(\"DayOfWeek\", dayofweek(col(\"date\")))\n",
    "variableA = (variableA.groupBy(weekofyear(\"date\")).agg(mean('actual_min_temp'),mean('actual_max_temp'),mean('average_min_temp'),mean('average_max_temp')).orderBy(weekofyear(\"date\")))\n",
    "variableA.coalesce(1).write.options(header='True').csv(\"C:/Users/dsvm_user/data/final/KCQT_weekly.csv\")\n",
    "vardf2 = variableA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframeVar = df3\n",
    "variableA = dataframeVar.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp')\n",
    "variableA = variableA.withColumn(\"DayOfWeek\", dayofweek(col(\"date\")))\n",
    "variableA = (variableA.groupBy(weekofyear(\"date\")).agg(mean('actual_min_temp'),mean('actual_max_temp'),mean('average_min_temp'),mean('average_max_temp')).orderBy(weekofyear(\"date\")))\n",
    "variableA.coalesce(1).write.options(header='True').csv(\"C:/Users/dsvm_user/data/final/KHOU_weekly.csv\")\n",
    "vardf3 = variableA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframeVar = df4\n",
    "variableA = dataframeVar.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp')\n",
    "variableA = variableA.withColumn(\"DayOfWeek\", dayofweek(col(\"date\")))\n",
    "variableA = (variableA.groupBy(weekofyear(\"date\")).agg(mean('actual_min_temp'),mean('actual_max_temp'),mean('average_min_temp'),mean('average_max_temp')).orderBy(weekofyear(\"date\")))\n",
    "variableA.coalesce(1).write.options(header='True').csv(\"C:/Users/dsvm_user/data/final/KIND_weekly.csv\")\n",
    "\n",
    "vardf4 = variableA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframeVar = df5\n",
    "variableA = dataframeVar.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp')\n",
    "variableA = variableA.withColumn(\"DayOfWeek\", dayofweek(col(\"date\")))\n",
    "variableA = (variableA.groupBy(weekofyear(\"date\")).agg(mean('actual_min_temp'),mean('actual_max_temp'),mean('average_min_temp'),mean('average_max_temp')).orderBy(weekofyear(\"date\")))\n",
    "variableA.coalesce(1).write.options(header='True').csv(\"C:/Users/dsvm_user/data/final/KJAX_weekly.csv\")\n",
    "vardf5 = variableA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframeVar = df6\n",
    "variableA = dataframeVar.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp')\n",
    "variableA = variableA.withColumn(\"DayOfWeek\", dayofweek(col(\"date\")))\n",
    "variableA = (variableA.groupBy(weekofyear(\"date\")).agg(mean('actual_min_temp'),mean('actual_max_temp'),mean('average_min_temp'),mean('average_max_temp')).orderBy(weekofyear(\"date\")))\n",
    "variableA.coalesce(1).write.options(header='True').csv(\"C:/Users/dsvm_user/data/final/KMDW_weekly.csv\")\n",
    "vardf6 = variableA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframeVar = df7\n",
    "variableA = dataframeVar.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp')\n",
    "variableA = variableA.withColumn(\"DayOfWeek\", dayofweek(col(\"date\")))\n",
    "variableA = (variableA.groupBy(weekofyear(\"date\")).agg(mean('actual_min_temp'),mean('actual_max_temp'),mean('average_min_temp'),mean('average_max_temp')).orderBy(weekofyear(\"date\")))\n",
    "variableA.coalesce(1).write.options(header='True').csv(\"C:/Users/dsvm_user/data/final/KNYC_weekly.csv\")\n",
    "vardf7 = variableA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframeVar = df8\n",
    "variableA = dataframeVar.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp')\n",
    "variableA = variableA.withColumn(\"DayOfWeek\", dayofweek(col(\"date\")))\n",
    "variableA = (variableA.groupBy(weekofyear(\"date\")).agg(mean('actual_min_temp'),mean('actual_max_temp'),mean('average_min_temp'),mean('average_max_temp')).orderBy(weekofyear(\"date\")))\n",
    "variableA.coalesce(1).write.options(header='True').csv(\"C:/Users/dsvm_user/data/final/KPHL_weekly.csv\")\n",
    "vardf8 = variableA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataframeVar = df9\n",
    "variableA = dataframeVar.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp')\n",
    "variableA = variableA.withColumn(\"DayOfWeek\", dayofweek(col(\"date\")))\n",
    "variableA = (variableA.groupBy(weekofyear(\"date\")).agg(mean('actual_min_temp'),mean('actual_max_temp'),mean('average_min_temp'),mean('average_max_temp')).orderBy(weekofyear(\"date\")))\n",
    "variableA.coalesce(1).write.options(header='True').csv(\"C:/Users/dsvm_user/data/final/KPHX_weekly.csv\")\n",
    "vardf9 = variableA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframeVar = df10\n",
    "variableA = dataframeVar.select('date','actual_min_temp','actual_max_temp','average_min_temp','average_max_temp')\n",
    "variableA = variableA.withColumn(\"DayOfWeek\", dayofweek(col(\"date\")))\n",
    "variableA = (variableA.groupBy(weekofyear(\"date\")).agg(mean('actual_min_temp'),mean('actual_max_temp'),mean('average_min_temp'),mean('average_max_temp')).orderBy(weekofyear(\"date\")))\n",
    "variableA.coalesce(1).write.options(header='True').csv(\"C:/Users/dsvm_user/data/final/KSEA_weekly.csv\")\n",
    "vardf10 = variableA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vardpass():\n",
    "    vardf,\n",
    "    vardf2,\n",
    "    vardf3,\n",
    "    vardf4,\n",
    "    vardf5,\n",
    "    vardf6,\n",
    "    vardf7,\n",
    "    vardf8,\n",
    "    vardf9,\n",
    "    vardf10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+--------------------+---------------------+---------------------+\n",
      "|weekofyear(date)|avg(actual_min_temp)|avg(actual_max_temp)|avg(average_min_temp)|avg(average_max_temp)|\n",
      "+----------------+--------------------+--------------------+---------------------+---------------------+\n",
      "|               1|  37.857142857142854|                53.0|                 30.0|   50.142857142857146|\n",
      "|               2|                20.0|   44.57142857142857|   29.285714285714285|                 50.0|\n",
      "|               3|  31.285714285714285|  49.714285714285715|                 29.0|   50.285714285714285|\n",
      "|               4|  32.857142857142854|  58.142857142857146|                 30.0|                 51.0|\n",
      "|               5|  27.857142857142858|                51.0|   30.142857142857142|   51.857142857142854|\n",
      "|               6|                28.0|   55.42857142857143|                 31.0|                 53.0|\n",
      "|               7|  28.571428571428573|  51.142857142857146|   32.285714285714285|    54.42857142857143|\n",
      "|               8|  21.142857142857142|  37.857142857142854|    33.57142857142857|   55.857142857142854|\n",
      "|               9|  29.857142857142858|   43.57142857142857|   35.142857142857146|   57.714285714285715|\n",
      "|              10|  33.857142857142854|                62.0|    36.57142857142857|   59.714285714285715|\n",
      "|              11|  49.285714285714285|   70.57142857142857|    38.42857142857143|   61.857142857142854|\n",
      "|              12|  44.142857142857146|   68.57142857142857|   40.142857142857146|                 64.0|\n",
      "|              13|   44.57142857142857|   64.42857142857143|   41.714285714285715|    66.28571428571429|\n",
      "|              14|  45.714285714285715|   73.28571428571429|    43.42857142857143|    68.28571428571429|\n",
      "|              15|   58.42857142857143|                79.0|   45.142857142857146|    70.42857142857143|\n",
      "|              16|                57.0|   69.85714285714286|                 47.0|    72.14285714285714|\n",
      "|              17|  47.714285714285715|   69.28571428571429|   48.857142857142854|    73.85714285714286|\n",
      "|              18|  44.142857142857146|   71.85714285714286|                 51.0|    75.42857142857143|\n",
      "|              19|  57.714285714285715|                83.0|                 53.0|    76.85714285714286|\n",
      "|              20|  61.857142857142854|   84.71428571428571|   55.285714285714285|    78.42857142857143|\n",
      "+----------------+--------------------+--------------------+---------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vardf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vardf = vardf.selectExpr(\"CAST('Row' AS STRING)\")\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "\n",
    "# SparkReader = spark.readStream \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#     .option(\"subscribe\", \"TestTopic2\") \\\n",
    "#     .option(\"startingOffsets\", \"earliest\")\n",
    "# SparkReader.load\n",
    "# SparkReader\n",
    "# DataA = spark.readStream \\\n",
    "#     .format(\"socket\") \\\n",
    "#     .option(\"host\", \"localhost\") \\\n",
    "#     .option(\"port\", \"9092\") \n",
    "\n",
    "# DataA.load \n",
    "\n",
    "\n",
    "# DataA.selectExpr(\"CAST('row' AS STRING)\")\\\n",
    "#    .write \\\n",
    "#    .format(\"kafka\") \\\n",
    "#    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#    .option(\"topic\", \"TestTopic2\") \\\n",
    "#    .save\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xval = spark.readStream \\\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#     .option(\"subscribe\", \"TestTopic6\") \\\n",
    "#     .option(\"startingOffsets\", \"latest\") \\\n",
    "#     .load()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StringType, MapType\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# pairs = dataframerdd1.map(lambda x: (x.split(\" \")[0], x))\n",
    "# dataframerdd1.mapValues(lambda x: (x, 1)).reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))\n",
    "# dataframerdd1 = MapType(StringType(),StringType(),False)\n",
    "\n",
    "# vardf.selectExpr(\"CAST('Row' AS STRING)\") \\\n",
    "#   .write \\\n",
    "#   .format(\"kafka\") \\\n",
    "#   .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "#   .option(\"subscribe\", \"TestTopic2\") \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kafka import SimpleProducer, KafkaClient\n",
    "# from kafka import KafkaProducer\n",
    "# from pyspark.streaming import StreamingContext\n",
    "# from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "# # Serialize json messages\n",
    "# import json\n",
    "# producer = KafkaProducer(bootstrap_servers='localhost:9092')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
